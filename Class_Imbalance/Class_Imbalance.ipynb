{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class_Imbalance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGIfQrtWPY36"
      },
      "source": [
        "# **Class Imbalance using imblearn**\n",
        "\n",
        "##### **We will discuss our class imbalance problem amongst our target variable 'isFraud'. Class imbalance problems often occur in these problems regarding fraudulent transaction identification and spam identification.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7-TM-r8PGX3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.utils import resample\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import time\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HavPSoytRL3C"
      },
      "source": [
        "### **The metric trap:**\n",
        "One of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n",
        "\n",
        "\n",
        "Some Metrics that can provide better insight than just accuracy include:\n",
        "\n",
        "* **Confusion Matrix**: a table showing correct predictions and types of incorrect predictions.\n",
        "\n",
        "* **Precision**: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n",
        "\n",
        "* **Recall**: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n",
        "\n",
        "* **F1 Score**: the weighted average of precision and recall.\n",
        "\n",
        "We will also be using a **Precision-Recall** curve, since there is a significant class imabalance between our feature and target variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ree1IqL7QCBd"
      },
      "source": [
        "### **Loading in and sorting the data after Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ersIezC1P7aQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff19ab6-de06-4022-f672-248d268b0355"
      },
      "source": [
        "df = pd.read_csv('full_data.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqNArdxESQNX"
      },
      "source": [
        "#### **Reducing memory usage (.py function)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUMKQXS-MosY"
      },
      "source": [
        "from memory_reduction import reduce_mem_usage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGZIMfaiSe9Z",
        "outputId": "715c3043-9ea9-4c59-e35b-47968985cd66"
      },
      "source": [
        "df = reduce_mem_usage(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage of dataframe is 1347.76 MB\n",
            "Memory usage after optimization is: 299.27 MB\n",
            "Decreased by 77.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FkyQG8RbG7U",
        "outputId": "eeb519f7-7d00-48c0-c863-52d57fa7a23a"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1097231, 161)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VmWvt-hz5es"
      },
      "source": [
        "**Resorting df to updated train and test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOFEAAetRyx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1584cb-f927-4a21-d6b2-a3e48e6f23a4"
      },
      "source": [
        "train, test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)\n",
        "\n",
        "#Dropping extra index column added when parsing\n",
        "train, test = train.drop('Unnamed: 0',axis=1) , test.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "#Converting all values of isFraud to numeric values of 0 or 1\n",
        "train['isFraud'] = train['isFraud'].apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(590540, 160)\n",
            "(506691, 159)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-nrAwhkU07F"
      },
      "source": [
        "#### **Looking at the distibution of target variable 'isFraud' in training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "6L-N33tyU8oY",
        "outputId": "8544a02e-248d-4f10-e112-a840a03be3e5"
      },
      "source": [
        "print('The percentage of fraudulent transactions in the training data set is', round(train['isFraud'].sum()/len(train['isFraud']) * 100,4),'%')\n",
        "print('Class Ratio:', round(sum(train['isFraud']/len(train['isFraud'])),5))\n",
        "print('\\nThere is a significant CLASS IMBALANCE probelem that must be addressed.')\n",
        "\n",
        "train.groupby('isFraud').count()['TransactionID'].plot(kind='barh',\n",
        "                                                      title='\\nDistribution of Target in Training set', color='r',figsize=(12, 3))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The percentage of fraudulent transactions in the training data set is 3.499 %\n",
            "Class Ratio: 0.03499\n",
            "\n",
            "There is a significant CLASS IMBALANCE probelem that must be addressed.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAADhCAYAAAAknBgsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVWElEQVR4nO3deZRtZXnn8e+PuRlkjmEQLigOJDEJEJSERO2kFaeYXnGABkFClr1MY6ejtg1K0pql6agLEg04ELVpxQHHKKhtcEjUNIr3oigYCUOQeTRMogTk6T/2U3Bu5dZ0b506Vfd+P2vtVfu8e593P2e/l8Ov9nn3qVQVkiRJkmCzSRcgSZIkLReGY0mSJKkZjiVJkqRmOJYkSZKa4ViSJElqhmNJkiSpGY4lSZKkZjiWJEmSmuFYkiRJaoZjSZIkqRmOJUmSpGY4liRJkprhWJIkSWqGY0mSJKkZjiVJkqRmOJYkSZKa4ViSJElqhmNJkiSpGY4lSZKkZjiWJEmSmuFYkiRJaoZjSeslyTuT/PEi9bVPknuSbN6P/y7J7y9G393f55Ict1j9LeC4b0hyW5KblvrYi6XHZf+VUsNyqFfSypaqmnQNkpaZJFcDjwQeAH4KfA94H3BmVT24Hn39flV9YQHP+Tvg7Kp690KO1c99HfCYqjpmoc9dTEn2AS4D9q2qW6ZtOxp4Vz/cHNgauHdqe1Vtv0Q1ngVcV1WnLHK/vw58buohsC3wo5FdDqyqaxbzmMvVhvxbljQZXjmWNJPnVtUOwL7AnwP/A3jPYh8kyRaL3ecysQ9w+/RgDFBVH6iq7TsEPxO4YerxQoLxcj13VfXVkdfyc92808hrfCgYL9fXIGnTZTiWNKuqurOqPg28CDguyc/DcNUxyRt6fbck5yW5I8kPk3w1yWZJ3s8QEs/tj7tfnWRVkkpyQpJrgC+NtI0GpUcnuTDJXUk+lWSXPtZTk1w3WmOSq5P8VpIjgNcAL+rjXdzbH5qm0XWdkuQHSW5J8r4kO/a2qTqOS3JNT4l47UznJsmO/fxbu79Tuv/fAs4H9uw6zprv+U5yUpIrk9yd5HtJ/uPItpck+Yckf5HkduB1SXZNcm6fp2/2VI6vjTzn8UnO73G5LMkLu/2lwNHAq7vGc2eop5I8ptfPSnJGks90fd9I8uj5vrbu43VJPpbk7CR3AS9JcmiSC/rfz41JTk+y1frUsMB9n97n5M4kb0/y95lhOk/XuLrP881JThvZ9uQk/6/rvzjJU7v9jcCvA6f3OT59IedK0mQYjiXNS1VdCFzH8D/76V7Z23ZnmI7xmuEp9WLgGoar0NtX1ZtHnvMU4AnAM2Y45LHA7wF7MEzveNs8avy/wJ8B5/TxfnEdu72kl6cB+wPbA9NDy+HA44DfBP4kyRNmOORfATt2P0/pmo/vKSSjV4RfMlftI65kOMc7Aq8Hzk6yx8j2JwFXMZznNwJnMExZ+FnguF4ASLIdQ0j/IPAzwJHA25McWFVnAh8A3tw1Pnee9R3Zde0MXNE1LNTzgI8BO3UNPwX+CNgNOIzhvP/BItWwzn2T7NY1nAzsyjAF5ldn6eetwFur6hHAo4GPdD97AZ8B3gDsArwK+HiS3avqtcBXgRP7HJ84S/+SlgnDsaSFuIEhAEx3P0OI3beq7u+P1ee6oeF1VfWjqvrxDNvfX1WXVNWPgD8GXpi+YW8DHQ2cVlVXVdU9DOHoyGlXrV9fVT+uqouBi4F/E7K7liOBk6vq7qq6GjgVePGGFFdVH62qG6rqwao6B7gcOHRklxuq6q+q6gHgX4HfBf5nVd1bVd8D/s/Ivs8Brq6q/11VD1TVt4CPAy/YgBI/WVUX9vE/APzSevRxQVX9Tb/GH1fVmqr6etd4NcN87KcsUg0z7fss4NKq+kRvexsw242T9wOPSbJbVd1TVV/v9mOAz1bVZ/v1nA+s7v4lrUCGY0kLsRfww3W0v4XhqtzfJrkqyUnz6OvaBWz/AbAlw5XFDbVn9zfa9xYMV2KnjIakexmuLk+3W9c0va+9NqS4JMcm+XZ/RH8H8POs/bpHz8vuXfu1M2zfF3jSVF/d39EMV5nX13zOzVzWGvskj80wLeemnmrxZ8w+1gupYaZ99xyto3+ZW2u6zjQnAI8Fvt/TV57T7fsCL5h2jg9n+GVR0gpkOJY0L0l+hSH4fW36tr5y+sqq2h/4beAVSX5zavMMXc51ZflRI+v7MFy5u41hCsG2I3VtzhAS59vvDQyBZrTvB4Cb53jedLd1TdP7un6B/Twkyb7AXwMnArtW1U7AJQzf+DBl9PXdylD73iNto+ftWuDvq2qnkWX7qnrZOvpaStOP+w7g+8ABPW3hNaz9msfhRkbOW5Kw9nlcS1VdXlVHMUxPeRPwsZ62ci3Dpxyj53i7qvrzqaeO7yVIGgfDsaRZJXlEXyX7MMNXUn13Hfs8J8ljOmDcyTCHdOor325mmJO7UMckOTDJtsCfAh+rqp8C/wRsk+TZSbYETmH4KrQpNwOrksz0/vYh4I+S7Jdkex6eo/zAQorrWj4CvDHJDh1sXwGcvZB+ptmOIUzdCpDkeIYrx7PV8AmGG/O2TfJ4hnnPU84DHpvkxUm27OVXRuZQr+/YLLYdgLuAe/o1vGyO/RfDZ4BfSPI7PaXmvzDLFfUkx/Q84geBO7r5QYbxfm6SZyTZPMk2GW4anQray+UcS5onw7GkmZyb5G6GK2OvBU4Djp9h3wOALwD3ABcAb6+qL/e2/wWc0h85v2oBx38/cBbDx+LbAP8Vhm/PYLhZ690MV2l/xNofh3+0f96e5KJ19Pve7vsrwD8DPwFevoC6Rr28j38VwxX1D3b/66XnDJ/KcA5vBn4B+Ic5nnYiw817NzG8rg8B93V/dwNPZ5gbfUPv8yYe/mXiPcCBPTZ/s751L4JXAf8JuJvhyvk54z5gVd3GMPf6zcDtwIEMc4Xvm+EpRwCXJrmH4ea8I3u+9LUMNxi+huGXmmuB/87D/399K/D8JP+SZM6bSiVNnn8ERJI2IkneBPxsVS35XwRcyfqThuuAo0d+sZO0CfLKsSStYBm+x/iJGRzKcOPYJydd10rQUyF2SrI1D89z/vocT5O0kfMvE0nSyrYDw1SKPRmmYpwKfGqiFa0chzFMhdmK4U+k/84sXy0oaRPhtApJkiSpOa1CkiRJaoZjSZIkqS2rOce77bZbrVq1atJlSJIkaSO2Zs2a26pq93VtW1bheNWqVaxevXrSZUiSJGkjluQHM21zWoUkSZLUDMeSJElSMxxLkiRJzXAsSZIkNcOxJEmS1AzHkiRJUjMcS5IkSc1wLEmSJDXDsSRJktQMx5IkSVIzHEuSJEnNcCxJkiQ1w7EkSZLUDMeSJElSMxxLkiRJzXAsSZIkNcOxJEmS1AzHkiRJUtti0gWsZc0aSMbTd9V4+pUkSdJGwyvHkiRJUjMcS5IkSc1wLEmSJDXDsSRJktQMx5IkSVIzHEuSJEnNcCxJkiQ1w7EkSZLUDMeSJElSMxxLkiRJzXAsSZIkNcOxJEmS1AzHkiRJUjMcS5IkSc1wLEmSJDXDsSRJktTGFo6TvDfJLUkuGdcxJEmSpMU0zivHZwFHjLF/SZIkaVGNLRxX1VeAH46rf0mSJGmxTXzOcZKXJlmdZPWtky5GkiRJm7SJh+OqOrOqDqmqQ3afdDGSJEnapE08HEuSJEnLheFYkiRJauP8KrcPARcAj0tyXZITxnUsSZIkaTFsMa6Oq+qocfUtSZIkjYPTKiRJkqRmOJYkSZKa4ViSJElqhmNJkiSpGY4lSZKkZjiWJEmSmuFYkiRJaoZjSZIkqRmOJUmSpGY4liRJkprhWJIkSWqGY0mSJKkZjiVJkqRmOJYkSZKa4ViSJElqyyscH3wwVI1nkSRJkuawvMKxJEmSNEGGY0mSJKkZjiVJkqRmOJYkSZLaFrNtTHLQbNur6qLFLUeSJEmanFnDMXBq/9wGOAS4GAjwRGA1cNj4SpMkSZKW1qzTKqrqaVX1NOBG4KCqOqSqDgZ+Gbh+KQqUJEmSlsp85xw/rqq+O/Wgqi4BnjCekiRJkqTJmGtaxZTvJHk3cHY/Phr4znhKkiRJkiZjvuH4eOBlwB/2468A7xhLRZIkSdKEzCscV9VPgL/oRZIkSdoozSscJ/lnoKa3V9X+i16RJEmSNCHznVZxyMj6NsALgF0WvxxJkiRpcub1bRVVdfvIcn1V/SXw7DHXJkmSJC2p+U6rGP1LeZsxXEme71VnSZIkaUWYb8A9dWT9AeBq4IWLXo0kSZI0QfP9toqnjbsQSZIkadLmPTUiybOBn2O4IQ+AqvrTcRQlSZIkTcK8bshL8k7gRcDLgTB8W8W+Y6xLkiRJWnLzCsfAr1bVscC/VNXrgcOAx46vLEmSJGnpzTcc/6R/3ptkT+B+YI/xlCRJkiRNxnznHJ+bZCfgLcBFDH8t76/HVpUkSZI0AXOG4ySbAV+sqjuAjyc5D9imqu4ce3WSJEnSEppzWkVVPQicMfL4PoOxJEmSNkbznXP8xSS/myRjrUaSJEmaoPmG4/8MfBS4L8ldSe5OctcY65IkSZKW3KxzjpM8uaq+XlU7LFVBkiRJ0qTMdeX47VMrSS4Ycy2SJEnSRM0VjkfnGG8z416SJEnSRmCur3LbLMnODCF6av2hwFxVPxxncZIkSdJSmisc7wis4eFAfNHItgL2H0dRkiRJ0iTMGo6ratUS1SFJkiRN3Ly+yi3JryXZrtePSXJakn3GW5okSZK0tOb7PcfvAO5N8ovAK4ErgfePrSpJkiRpAuYbjh+oqgKeB5xeVWcAfvexJEmSNipz3ZA35e4kJwPHAL+RZDNgy/GVJUmSJC29+V45fhFwH3BCVd0E7A28ZWxVSZIkSRMwryvHHYhPG3l8DfC+cRUlSZIkTcKs4TjJ16rq8CR3M3yv8UObgKqqR4y1OkmSJGkJzfU9x4f3T2++kyRJ0kZvvjfkLY01ayCZez9JkiStXFVz7zMh870hT5IkSdroGY4lSZKkZjiWJEmSmuFYkiRJaoZjSZIkqRmOJUmSpGY4liRJkprhWJIkSWqGY0mSJKkZjiVJkqRmOJYkSZKa4ViSJElqhmNJkiSpGY4lSZKkZjiWJEmSmuFYkiRJamMNx0mOSHJZkiuSnDTOY0mSJEkbamzhOMnmwBnAM4EDgaOSHDiu40mSJEkbapxXjg8Frqiqq6rqX4EPA88b4/EkSZKkDTLOcLwXcO3I4+u6bS1JXppkdZLVt46xGEmSJGkuE78hr6rOrKpDquqQ3SddjCRJkjZp4wzH1wOPGnm8d7dJkiRJy9I4w/E3gQOS7JdkK+BI4NNjPJ4kSZK0QbYYV8dV9UCSE4HPA5sD762qS8d1PEmSJGlDjS0cA1TVZ4HPjvMYkiRJ0mKZ+A15kiRJ0nJhOJYkSZKa4ViSJElqhmNJkiSpGY4lSZKkZjiWJEmSmuFYkiRJaoZjSZIkqRmOJUmSpGY4liRJkprhWJIkSWqGY0mSJKkZjiVJkqRmOJYkSZLaFpMuYC0HHwyrV0+6CkmSJG2ivHIsSZIkNcOxJEmS1AzHkiRJUjMcS5IkSc1wLEmSJDXDsSRJktQMx5IkSVIzHEuSJEnNcCxJkiQ1w7EkSZLUDMeSJElSMxxLkiRJzXAsSZIkNcOxJEmS1AzHkiRJUjMcS5IkSc1wLEmSJDXDsSRJktQMx5IkSVIzHEuSJEktVTXpGh6S5G7gsknXofWyG3DbpIvQenP8VjbHb2Vz/FY2x29l2reqdl/Xhi2WupI5XFZVh0y6CC1cktWO3crl+K1sjt/K5vitbI7fxsdpFZIkSVIzHEuSJEltuYXjMyddgNabY7eyOX4rm+O3sjl+K5vjt5FZVjfkSZIkSZO03K4cS5IkSROzLMJxkiOSXJbkiiQnTbqeTU2S9ya5JcklI227JDk/yeX9c+duT5K39Vh9J8lBI885rve/PMlxI+0HJ/luP+dtSTLbMTR/SR6V5MtJvpfk0iR/2O2O3wqQZJskFya5uMfv9d2+X5Jv9Dk/J8lW3b51P76it68a6evkbr8syTNG2tf5/jrTMbRwSTZP8q0k5/Vjx2+FSHJ1v799O8nqbvP9c1NXVRNdgM2BK4H9ga2Ai4EDJ13XprQAvwEcBFwy0vZm4KRePwl4U68/C/gcEODJwDe6fRfgqv65c6/v3Nsu7H3Tz33mbMdwWdDY7QEc1Os7AP8EHOj4rYylz+n2vb4l8I0+1x8Bjuz2dwIv6/U/AN7Z60cC5/T6gf3euTWwX7+nbj7b++tMx3BZr3F8BfBB4LzZzq3jt/wW4Gpgt2ltvn9u4svkC4DDgM+PPD4ZOHnSdW1qC7CKtcPxZcAevb4Hw3dQA7wLOGr6fsBRwLtG2t/VbXsA3x9pf2i/mY7hskHj+CngPzh+K28BtgUuAp7E8AcFtuj2h94jgc8Dh/X6Fr1fpr9vTu030/trP2edx3BZ8LjtDXwR+PfAebOdW8dv+S2sOxz7/rmJL8thWsVewLUjj6/rNk3WI6vqxl6/CXhkr880XrO1X7eO9tmOofXQH9H+MsPVR8dvheiP5L8N3AKcz3Cl8I6qeqB3GT3nD41Tb78T2JWFj+uusxxDC/OXwKuBB/vxbOfW8Vt+CvjbJGuSvLTbfP/cxC23v5CnZaiqKslYv9ZkKY6xMUuyPfBx4L9V1V09rQ1w/Ja7qvop8EtJdgI+CTx+wiVpnpI8B7ilqtYkeeqk69F6Obyqrk/yM8D5Sb4/utH3z03TcrhyfD3wqJHHe3ebJuvmJHsA9M9bun2m8Zqtfe91tM92DC1Aki0ZgvEHquoT3ez4rTBVdQfwZYaPyHdKMnXxYvScPzROvX1H4HYWPq63z3IMzd+vAb+d5GrgwwxTK96K47diVNX1/fMWhl9OD8X3z03ecgjH3wQO6Dtvt2K4SeHTE65JwxhM3XF7HMNc1qn2Y/uu3ScDd/ZHQ58Hnp5k577r9ukMc+BuBO5K8uS+S/fYaX2t6xiapz6n7wH+sapOG9nk+K0ASXbvK8Yk+XcM88X/kSEkP793mz5+U+f8+cCXqqq6/cj+NoT9gAMYbgRa5/trP2emY2iequrkqtq7qlYxnNsvVdXROH4rQpLtkuwwtc7wvncJvn9q0pOeh//GeRbDXfZXAq+ddD2b2gJ8CLgRuJ9hTtQJDHPavghcDnwB2KX3DXBGj9V3gUNG+vk94Ipejh9pP4ThDedK4HQe/uMz6zyGy4LG7nCGOXPfAb7dy7Mcv5WxAE8EvtXjdwnwJ92+P0M4ugL4KLB1t2/Tj6/o7fuP9PXaHqPL6Dviu32d768zHcNlvcfyqTz8bRWO3wpY+hxe3MulU+fX908X/0KeJEmS1JbDtApJkiRpWTAcS5IkSc1wLEmSJDXDsSRJktQMx5IkSVIzHEuSJEnNcCxJkiQ1w7EkSZLU/j/cBR14mmYhNQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oOklUxfzw0D"
      },
      "source": [
        "**Sorting X_train, y_train, and X_test by TransactionDT, timedelta feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqCgwDX0s6ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c325950f-8233-4510-b057-2ca8364250c0"
      },
      "source": [
        "X_train = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', ],axis=1)\n",
        "#y_train = train.sort_values('TransactionDT')['isFraud'].astype(bool)\n",
        "y_train = train.sort_values('TransactionDT')['isFraud']\n",
        "\n",
        "X_test = test.sort_values('TransactionDT').drop(['TransactionDT', ], axis=1)\n",
        "\n",
        "print('X_train shape: {}'.format(X_train.shape))\n",
        "print('y_train shape: {}'.format(y_train.shape))\n",
        "print('X_test shape: {}'.format(X_test.shape))\n",
        "\n",
        "del train\n",
        "test = test[[\"TransactionDT\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (590540, 158)\n",
            "y_train shape: (590540,)\n",
            "X_test shape: (506691, 158)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBSH01iWOi4t"
      },
      "source": [
        "**Cleaning X_train, y_train, and X_test datasets by replacing NaN values with -999**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzUO2yToOKnE"
      },
      "source": [
        "X_train.fillna(-999, inplace=True)\n",
        "y_train.fillna(-999, inplace=True)\n",
        "X_test.fillna( -999, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bXqolAUHWm"
      },
      "source": [
        "## **Resampling methods** *(Over/ Undersampling for Class imbalance)*\n",
        "* **We will only be applying these resampling methods on the training dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2QX0kGXURZV"
      },
      "source": [
        "1. **Oversample minority class**\n",
        "2. **Undersample majority class**\n",
        "3. **Random over-sampling with python module: imbalanced-learn (This is what we will be using for our final dataset to prepare it for modeling)** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrQyKtQPPmcS"
      },
      "source": [
        "# Joining our X_train and y_train data to perform resampling\n",
        "full = pd.concat([X_train,y_train],axis=1)\n",
        "not_fraud = full[full.isFraud==0]\n",
        "fraud = full[full.isFraud==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcAUZa8xY0cq"
      },
      "source": [
        "### **1. Oversampling the minority class**\n",
        "\n",
        "* Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don’t have a ton of data to work with.\n",
        "* We will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class (isFraud)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn1ckKckUNwK",
        "outputId": "913fb667-bd29-4f4e-d14a-e5621178e46f"
      },
      "source": [
        "# 1. Oversampling the minority class\n",
        "\n",
        "# upsample minority\n",
        "fraud_upsampled = resample(fraud,\n",
        "                          replace=True, # sample with replacement\n",
        "                          n_samples=len(not_fraud), # match number in majority class\n",
        "                          random_state=27) # reproducible results\n",
        "\n",
        "# combine majority and upsampled minority\n",
        "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
        "\n",
        "# check new class counts\n",
        "upsampled.isFraud.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    569877\n",
              "0    569877\n",
              "Name: isFraud, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3tdHYrHaLnL"
      },
      "source": [
        "### **2. Undersampling the majority class**\n",
        "* Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\n",
        "* We will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class. This method is not paticularly useful in our case, since we want to **increase** instances of the the target variable 'isFraud', which is the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx7_MieAaFh3",
        "outputId": "d702cd6d-b7f4-4963-eaf0-e6c213f741a5"
      },
      "source": [
        "# 2. Undersampling the majority class\n",
        "\n",
        "#downsample majority\n",
        "not_fraud_downsampled = resample(not_fraud,\n",
        "                                replace = False, # sample without replacement\n",
        "                                n_samples = len(fraud), # match minority n\n",
        "                                random_state = 27) # reproducible results\n",
        "\n",
        "# combine minority and downsampled majority\n",
        "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
        "\n",
        "# checking counts (1- True, 0- False)\n",
        "downsampled.isFraud.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    20663\n",
              "0    20663\n",
              "Name: isFraud, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU8kSmQSqKW_"
      },
      "source": [
        "## **IMBLEARN:**\n",
        "* imblearn is a much more sophisticated resampling technique. Using imblearn we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooyrTAM0pSOs"
      },
      "source": [
        "### **3. Random Over-sampling with imblearn**\n",
        "* #### **This is the method we will be using for our training dataset, as we want to increase instances of isFraud = 1, which will be achieved by oversampling the minority class of the target variable.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XtIiXFIbVHO",
        "outputId": "59e746b3-27a1-45ac-cab5-f9701402e172"
      },
      "source": [
        "import imblearn\n",
        "from collections import Counter\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qj7kM9CrrJV",
        "outputId": "f65f2728-dddd-4720-c6aa-9d4b8e6f48e8"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Distribution prior to resampling \n",
        "print('Distribtuion prior sampling: ', Counter(y_train))\n",
        "\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "X_samp, y_samp = oversample.fit_resample(X_train,y_train)\n",
        "\n",
        "# Distribution after resmpling\n",
        "print('Distribution after sampling: ', Counter(y_samp))\n",
        "\n",
        "print('\\nThe new data now contains {} rows '.format(X_samp.shape[0]))\n",
        "\n",
        "# Plotting the new vs. old distributions of the training data\n",
        "#plot_2d_space(X_rand,y_rand, X_train.ravel(), y_train, 'Random over sampling')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribtuion prior sampling:  Counter({0: 569877, 1: 20663})\n",
            "Distribution after sampling:  Counter({0: 569877, 1: 569877})\n",
            "\n",
            "The new data now contains 1139754 rows \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "-ek7a567QaUV",
        "outputId": "669f475d-701c-4377-acc8-3a82e7bf7780"
      },
      "source": [
        "X_resampled = pd.DataFrame(X_samp, columns=X_train.columns)\n",
        "y_resampled = pd.DataFrame(y_samp).rename(columns = {0: 'isFraud'})\n",
        "X_resampled.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>D10</th>\n",
              "      <th>D11</th>\n",
              "      <th>D12</th>\n",
              "      <th>D14</th>\n",
              "      <th>D15</th>\n",
              "      <th>M1</th>\n",
              "      <th>...</th>\n",
              "      <th>C1_to_mean_addr2</th>\n",
              "      <th>C1_to_std_addr2</th>\n",
              "      <th>C1_to_mean_dist1</th>\n",
              "      <th>C1_to_std_dist1</th>\n",
              "      <th>C2_to_mean_addr1</th>\n",
              "      <th>C2_to_std_addr1</th>\n",
              "      <th>C2_to_mean_addr2</th>\n",
              "      <th>C2_to_std_addr2</th>\n",
              "      <th>C2_to_mean_dist1</th>\n",
              "      <th>C2_to_std_dist1</th>\n",
              "      <th>PCA_V_0</th>\n",
              "      <th>PCA_V_1</th>\n",
              "      <th>PCA_V_2</th>\n",
              "      <th>PCA_V_3</th>\n",
              "      <th>PCA_V_4</th>\n",
              "      <th>PCA_V_5</th>\n",
              "      <th>PCA_V_6</th>\n",
              "      <th>PCA_V_7</th>\n",
              "      <th>PCA_V_8</th>\n",
              "      <th>PCA_V_9</th>\n",
              "      <th>PCA_V_10</th>\n",
              "      <th>PCA_V_11</th>\n",
              "      <th>PCA_V_12</th>\n",
              "      <th>PCA_V_13</th>\n",
              "      <th>PCA_V_14</th>\n",
              "      <th>PCA_V_15</th>\n",
              "      <th>PCA_V_16</th>\n",
              "      <th>PCA_V_17</th>\n",
              "      <th>PCA_V_18</th>\n",
              "      <th>PCA_V_19</th>\n",
              "      <th>PCA_V_20</th>\n",
              "      <th>PCA_V_21</th>\n",
              "      <th>PCA_V_22</th>\n",
              "      <th>PCA_V_23</th>\n",
              "      <th>PCA_V_24</th>\n",
              "      <th>PCA_V_25</th>\n",
              "      <th>PCA_V_26</th>\n",
              "      <th>PCA_V_27</th>\n",
              "      <th>PCA_V_28</th>\n",
              "      <th>PCA_V_29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000.0</td>\n",
              "      <td>4.226562</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3417.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.104919</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>0.120422</td>\n",
              "      <td>0.036011</td>\n",
              "      <td>0.130859</td>\n",
              "      <td>0.029251</td>\n",
              "      <td>0.110535</td>\n",
              "      <td>0.018448</td>\n",
              "      <td>0.134644</td>\n",
              "      <td>0.040741</td>\n",
              "      <td>0.374756</td>\n",
              "      <td>0.909180</td>\n",
              "      <td>0.159180</td>\n",
              "      <td>0.896973</td>\n",
              "      <td>0.189331</td>\n",
              "      <td>-0.027542</td>\n",
              "      <td>0.031525</td>\n",
              "      <td>-0.173096</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.024216</td>\n",
              "      <td>-0.018219</td>\n",
              "      <td>-0.034332</td>\n",
              "      <td>-0.123291</td>\n",
              "      <td>0.013222</td>\n",
              "      <td>-0.014397</td>\n",
              "      <td>0.076172</td>\n",
              "      <td>0.032654</td>\n",
              "      <td>0.012947</td>\n",
              "      <td>-0.012138</td>\n",
              "      <td>-0.008698</td>\n",
              "      <td>-0.005699</td>\n",
              "      <td>-0.002466</td>\n",
              "      <td>0.018723</td>\n",
              "      <td>-0.005238</td>\n",
              "      <td>0.006531</td>\n",
              "      <td>-0.045319</td>\n",
              "      <td>-0.034210</td>\n",
              "      <td>-0.053741</td>\n",
              "      <td>0.026672</td>\n",
              "      <td>-0.060516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001.0</td>\n",
              "      <td>3.367188</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7922.0</td>\n",
              "      <td>303.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.104919</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.124390</td>\n",
              "      <td>0.021454</td>\n",
              "      <td>0.110535</td>\n",
              "      <td>0.018448</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.057159</td>\n",
              "      <td>-0.172119</td>\n",
              "      <td>0.629883</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>-0.438477</td>\n",
              "      <td>0.038239</td>\n",
              "      <td>-0.096375</td>\n",
              "      <td>-0.050079</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.005173</td>\n",
              "      <td>0.077026</td>\n",
              "      <td>-0.130981</td>\n",
              "      <td>-0.085510</td>\n",
              "      <td>-0.043732</td>\n",
              "      <td>-0.016922</td>\n",
              "      <td>0.015350</td>\n",
              "      <td>0.003777</td>\n",
              "      <td>0.017746</td>\n",
              "      <td>-0.012070</td>\n",
              "      <td>0.001741</td>\n",
              "      <td>-0.039368</td>\n",
              "      <td>0.003017</td>\n",
              "      <td>0.012375</td>\n",
              "      <td>-0.002649</td>\n",
              "      <td>-0.004078</td>\n",
              "      <td>0.008934</td>\n",
              "      <td>0.029266</td>\n",
              "      <td>0.029114</td>\n",
              "      <td>-0.023621</td>\n",
              "      <td>-0.037689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002.0</td>\n",
              "      <td>4.078125</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9383.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>315.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>315.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.104919</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>0.456299</td>\n",
              "      <td>0.753418</td>\n",
              "      <td>0.116455</td>\n",
              "      <td>0.017685</td>\n",
              "      <td>0.110535</td>\n",
              "      <td>0.018448</td>\n",
              "      <td>0.567871</td>\n",
              "      <td>0.957520</td>\n",
              "      <td>-0.347168</td>\n",
              "      <td>0.210938</td>\n",
              "      <td>0.766602</td>\n",
              "      <td>0.227051</td>\n",
              "      <td>0.069214</td>\n",
              "      <td>-0.025726</td>\n",
              "      <td>0.083496</td>\n",
              "      <td>-0.176514</td>\n",
              "      <td>0.059845</td>\n",
              "      <td>0.017334</td>\n",
              "      <td>-0.021301</td>\n",
              "      <td>-0.012100</td>\n",
              "      <td>0.003212</td>\n",
              "      <td>0.010880</td>\n",
              "      <td>-0.042267</td>\n",
              "      <td>0.017624</td>\n",
              "      <td>-0.011185</td>\n",
              "      <td>-0.001363</td>\n",
              "      <td>-0.016678</td>\n",
              "      <td>-0.000590</td>\n",
              "      <td>-0.009560</td>\n",
              "      <td>0.002848</td>\n",
              "      <td>-0.003054</td>\n",
              "      <td>0.002321</td>\n",
              "      <td>-0.005459</td>\n",
              "      <td>-0.001485</td>\n",
              "      <td>-0.000145</td>\n",
              "      <td>0.001475</td>\n",
              "      <td>0.004406</td>\n",
              "      <td>0.000626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003.0</td>\n",
              "      <td>3.912109</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6991.0</td>\n",
              "      <td>466.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>282.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.209839</td>\n",
              "      <td>0.038757</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.685547</td>\n",
              "      <td>0.101257</td>\n",
              "      <td>0.552734</td>\n",
              "      <td>0.092224</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.190918</td>\n",
              "      <td>-0.130981</td>\n",
              "      <td>0.684082</td>\n",
              "      <td>0.438232</td>\n",
              "      <td>-0.424805</td>\n",
              "      <td>0.055695</td>\n",
              "      <td>-0.041962</td>\n",
              "      <td>-0.176880</td>\n",
              "      <td>0.165771</td>\n",
              "      <td>0.064514</td>\n",
              "      <td>-0.032745</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.035736</td>\n",
              "      <td>-0.043671</td>\n",
              "      <td>-0.014610</td>\n",
              "      <td>-0.023804</td>\n",
              "      <td>-0.026154</td>\n",
              "      <td>0.034119</td>\n",
              "      <td>0.134399</td>\n",
              "      <td>-0.053589</td>\n",
              "      <td>0.063416</td>\n",
              "      <td>0.024902</td>\n",
              "      <td>0.013138</td>\n",
              "      <td>0.026688</td>\n",
              "      <td>-0.002556</td>\n",
              "      <td>0.027267</td>\n",
              "      <td>0.053802</td>\n",
              "      <td>0.033844</td>\n",
              "      <td>-0.015182</td>\n",
              "      <td>-0.017883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004.0</td>\n",
              "      <td>3.912109</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9262.0</td>\n",
              "      <td>413.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.104919</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.116882</td>\n",
              "      <td>0.013092</td>\n",
              "      <td>0.110535</td>\n",
              "      <td>0.018448</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>2.810547</td>\n",
              "      <td>-0.055969</td>\n",
              "      <td>0.622070</td>\n",
              "      <td>-0.219727</td>\n",
              "      <td>0.266602</td>\n",
              "      <td>1.117188</td>\n",
              "      <td>-0.074158</td>\n",
              "      <td>-0.132202</td>\n",
              "      <td>-0.293945</td>\n",
              "      <td>-0.086243</td>\n",
              "      <td>-0.016876</td>\n",
              "      <td>0.077637</td>\n",
              "      <td>-0.168579</td>\n",
              "      <td>0.015015</td>\n",
              "      <td>-0.001257</td>\n",
              "      <td>0.004368</td>\n",
              "      <td>-0.000573</td>\n",
              "      <td>-0.020493</td>\n",
              "      <td>0.006176</td>\n",
              "      <td>-0.011879</td>\n",
              "      <td>0.013397</td>\n",
              "      <td>-0.011978</td>\n",
              "      <td>0.007103</td>\n",
              "      <td>0.074768</td>\n",
              "      <td>0.026657</td>\n",
              "      <td>0.023148</td>\n",
              "      <td>-0.041290</td>\n",
              "      <td>-0.004898</td>\n",
              "      <td>-0.000383</td>\n",
              "      <td>0.008095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 158 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  TransactionAmt  ProductCD  ...  PCA_V_27  PCA_V_28  PCA_V_29\n",
              "0      2987000.0        4.226562        4.0  ... -0.053741  0.026672 -0.060516\n",
              "1      2987001.0        3.367188        4.0  ...  0.029114 -0.023621 -0.037689\n",
              "2      2987002.0        4.078125        4.0  ...  0.001475  0.004406  0.000626\n",
              "3      2987003.0        3.912109        4.0  ...  0.033844 -0.015182 -0.017883\n",
              "4      2987004.0        3.912109        1.0  ... -0.004898 -0.000383  0.008095\n",
              "\n",
              "[5 rows x 158 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_PcrbnwaTxh"
      },
      "source": [
        "## **SAVE resampled data for modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4T_oQEcbQpl"
      },
      "source": [
        "**Training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGpFe14qaSwy"
      },
      "source": [
        "X_resampled.to_csv('X_train.csv')\n",
        "y_resampled.to_csv('y_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okb0f8g2bVAA"
      },
      "source": [
        "**Testing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44GMp_CKYwBf"
      },
      "source": [
        "X_test.to_csv('X_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}